---
title: DIA-07 - Ciberseguridad en Modelos de Inteligencia Artificial
description: Control de seguridad en el ciclo de vida de modelos de IA/ML y protecci√≥n de datos de entrenamiento
---

## üéØ Objetivo del Control

Establecer y mantener controles de seguridad para el desarrollo, entrenamiento, despliegue y operaci√≥n de modelos de inteligencia artificial y machine learning, protegiendo los datos de entrenamiento, mitigando sesgos, previniendo ataques adversariales y garantizando transparencia y responsabilidad.

## üìã Descripci√≥n

Este control aborda riesgos espec√≠ficos de IA/ML:
- Seguridad de datos de entrenamiento y datasets
- Protecci√≥n de modelos y propiedad intelectual
- Ataques adversariales (evasion, poisoning, model inversion)
- Sesgo algor√≠tmico y fairness
- Privacidad y fuga de informaci√≥n de entrenamiento
- Explicabilidad y auditor√≠a de decisiones
- Gobernanza de modelos en producci√≥n

## üõ°Ô∏è Controles Requeridos

### Seguridad en Datos de Entrenamiento

#### Calidad y Procedencia de Datos
- **Data Provenance:**
  - Registro de origen de datasets
  - Cadena de custodia de datos
  - Versionado de datasets (DVC - Data Version Control)
  - Metadatos de recolecci√≥n y procesamiento
- **Data Quality:**
  - Validaci√≥n de completitud y exactitud
  - Detecci√≥n de outliers y anomal√≠as
  - Limpieza y normalizaci√≥n documentada
  - Evaluaci√≥n de representatividad y balance
- **Data Sanitization:**
  - Detecci√≥n y eliminaci√≥n de datos t√≥xicos o maliciosos
  - Filtrado de contenido inapropiado
  - Desduplicaci√≥n de registros

#### Protecci√≥n de Datos Sensibles
- **Privacidad en Entrenamiento:**
  - Anonimizaci√≥n/pseudonimizaci√≥n de PII
  - Differential Privacy (Œµ-differential privacy, Œ¥ < 0.001)
  - Federated Learning para datos distribuidos sin centralizaci√≥n
  - Synthetic data generation para casos de uso no cr√≠ticos
- **Data Minimization:**
  - Uso de datos estrictamente necesarios
  - Feature selection y dimensionality reduction
  - Eliminaci√≥n de atributos sensibles innecesarios
- **Cumplimiento GDPR/LGPD:**
  - Consentimiento para uso en IA cuando aplique
  - Data Protection Impact Assessment (DPIA) para IA de alto riesgo
  - Documentaci√≥n de base legal para procesamiento

### Seguridad del Modelo

#### Model Security
- **Protecci√≥n de Propiedad Intelectual:**
  - Cifrado de modelos almacenados
  - Control de acceso a model registry (MLflow, W&B, Neptune)
  - Watermarking de modelos
  - Ofuscaci√≥n de arquitectura en edge deployment
- **Model Versioning:**
  - Registro de todas las versiones de modelos
  - Trazabilidad de hiperpar√°metros y configuraciones
  - Reproducibilidad de experimentos (MLOps)
  - Rollback capability
- **Supply Chain Security:**
  - Verificaci√≥n de integridad de modelos pre-entrenados
  - Scanning de dependencias (Dependabot, Snyk)
  - Uso de modelos de fuentes confiables (Hugging Face verified)
  - Checksum y firmas digitales de artefactos

#### Ataques Adversariales

**Data Poisoning:**
- **Backdoor Attacks:**
  - Detecci√≥n de triggers o patrones maliciosos en datos
  - Validaci√≥n de datasets de terceros
  - Auditor√≠a de proveedores de datos
- **Label Flipping:**
  - Validaci√≥n de labels mediante multiple annotators
  - Detecci√≥n de inconsistencias estad√≠sticas
  - Cross-validation rigurosa

**Model Evasion (Inference Attacks):**
- **Adversarial Examples:**
  - Testing con frameworks de adversarial attacks (CleverHans, Foolbox, ART)
  - Adversarial training (reentrenamiento con ejemplos adversariales)
  - Input validation y sanitization
  - Ensemble methods y randomization
- **Model Extraction:**
  - Rate limiting en APIs de inferencia
  - Detecci√≥n de queries sospechosos (volumen, patrones)
  - Watermarking de predicciones
  - API authentication obligatoria

**Model Inversion / Membership Inference:**
- **Fuga de Informaci√≥n de Entrenamiento:**
  - Differential Privacy en entrenamiento
  - Regularizaci√≥n agresiva (dropout, L2)
  - Limitaci√≥n de confianza en outputs (temperature scaling)
  - Auditor√≠a de susceptibilidad a membership inference

### Sesgo y Fairness

#### Detecci√≥n de Sesgo
- **M√©tricas de Fairness:**
  - Demographic Parity
  - Equalized Odds
  - Predictive Parity
  - Individual Fairness
- **Testing:**
  - Evaluaci√≥n en subgrupos demogr√°ficos
  - An√°lisis de disparate impact
  - Fairness audits peri√≥dicos
- **Herramientas:**
  - AI Fairness 360 (IBM)
  - Fairlearn (Microsoft)
  - What-If Tool (Google)

#### Mitigaci√≥n de Sesgo
- **Pre-processing:**
  - Reweighting de samples
  - Resampling y synthetic minority oversampling (SMOTE)
- **In-processing:**
  - Adversarial debiasing
  - Prejudice remover
  - Constrained optimization
- **Post-processing:**
  - Threshold optimization por grupo
  - Calibration
  - Reject option classification

### Explicabilidad y Transparencia

#### Interpretability
- **Model-Agnostic Methods:**
  - SHAP (SHapley Additive exPlanations)
  - LIME (Local Interpretable Model-agnostic Explanations)
  - Partial Dependence Plots
  - Feature importance
- **Model-Specific:**
  - Attention mechanisms en transformers
  - Decision tree visualization
  - Coefficient analysis en modelos lineales
- **Documentaci√≥n:**
  - Model cards (Google)
  - Datasheets for datasets (Microsoft)
  - Factsheets (IBM)

#### Auditor√≠a de Decisiones
- Logging de inferencias con contexto
- Explicaciones almacenadas por decisi√≥n cr√≠tica
- Capacidad de contestar apelaciones
- Human-in-the-loop para decisiones de alto impacto

### Gobernanza de Modelos (MLOps Security)

#### Desarrollo Seguro
- **Secure ML Pipeline:**
  - Code review de notebooks y scripts
  - Secrets management (no hardcoded credentials)
  - SAST/DAST en c√≥digo de ML
  - Container scanning en im√°genes de entrenamiento
- **Experiment Tracking:**
  - Registro centralizado (MLflow, W&B)
  - Auditor√≠a de accesos a experimentos
  - Segregaci√≥n por proyecto/equipo

#### Despliegue Seguro
- **Model Registry:**
  - Aprobaci√≥n antes de promoci√≥n a producci√≥n
  - Staging environment obligatorio
  - A/B testing y canary deployments
  - Automated rollback en degradaci√≥n de m√©tricas
- **Inference Security:**
  - API authentication y authorization
  - Rate limiting y throttling
  - Input validation (type, range, format)
  - Output sanitization
  - DDoS protection

#### Monitoreo en Producci√≥n
- **Model Monitoring:**
  - Data drift detection (distribution shifts)
  - Concept drift detection (cambio en relaci√≥n X-Y)
  - Model performance degradation
  - Bias drift (cambio en fairness metrics)
- **Alertas:**
  - Ca√≠da en accuracy/F1 score
  - Aumento en latencia de inferencia
  - Distribuci√≥n de inputs fuera de lo esperado
  - Detecci√≥n de adversarial patterns
- **Incident Response:**
  - Playbook para degradaci√≥n de modelo
  - Procedimiento de rollback r√°pido
  - Comunicaci√≥n a stakeholders

### Cumplimiento Regulatorio de IA

#### AI Act (Uni√≥n Europea)
- **Clasificaci√≥n de Riesgo:**
  - IA Prohibida (social scoring, manipulaci√≥n)
  - IA de Alto Riesgo (salud, seguridad, infraestructura cr√≠tica)
  - IA de Riesgo Limitado (chatbots - transparencia requerida)
  - IA de Riesgo M√≠nimo
- **Requisitos para IA de Alto Riesgo:**
  - Risk management system
  - Data governance y calidad
  - Documentaci√≥n t√©cnica completa
  - Transparencia y provisi√≥n de informaci√≥n a usuarios
  - Supervisi√≥n humana
  - Accuracy, robustez, ciberseguridad

#### Otros Marcos Regulatorios
- **NIST AI Risk Management Framework (AI RMF):**
  - Govern, Map, Measure, Manage
- **ISO/IEC 42001 - AI Management System**
- **Algorithmic Accountability Acts (USA)**

## üìä M√©tricas e Indicadores

- Modelos en producci√≥n con documentaci√≥n completa (model cards) (objetivo: 100%)
- Cobertura de testing adversarial (objetivo: 100% modelos cr√≠ticos)
- Incidentes de sesgo algor√≠tmico detectados y remediados
- Tiempo medio de detecci√≥n de data drift (MTTD)
- Porcentaje de modelos con monitoreo de fairness activo
- Vulnerabilidades cr√≠ticas en dependencias de ML (objetivo: 0)
- Modelos con differential privacy implementado (para datos sensibles)
- Solicitudes de explicaci√≥n de decisiones atendidas en SLA
- Auditor√≠as de modelos de alto riesgo (objetivo: anual)

## üîó Herramientas Recomendadas

- **MLOps Platforms:** MLflow, Weights & Biases, Neptune.ai, Kubeflow
- **Model Registry:** MLflow Model Registry, AWS SageMaker Model Registry
- **Adversarial Testing:** CleverHans, Foolbox, Adversarial Robustness Toolbox (ART)
- **Fairness:** AI Fairness 360, Fairlearn, Aequitas
- **Explainability:** SHAP, LIME, InterpretML, Captum
- **Privacy:** PySyft (Federated Learning), Opacus (Differential Privacy), TensorFlow Privacy
- **Data Versioning:** DVC, Pachyderm, LakeFS
- **Monitoring:** Evidently AI, WhyLabs, Fiddler AI, Arize AI
- **Security Scanning:** Snyk, Dependabot, Trivy, Aqua Security

## üìö Referencias

- [Pol√≠tica de Desarrollo Seguro](/docs/kudo/politicas/politica-desarrollo-seguro)
- [Pol√≠tica de Protecci√≥n de Datos Personales](/docs/kudo/politicas/politica-proteccion-datos-personales)
- [Pol√≠tica de Clasificaci√≥n de Informaci√≥n](/docs/kudo/politicas/clasificacion-informacion)
- [DIA-01: Inventario de Datos](/docs/kudo/dia-ciberseguridad-en-datos-e-inteligencia-artificial/dia-01)
- [DIA-04: Ciberseguridad en Informaci√≥n y Datos](/docs/kudo/dia-ciberseguridad-en-datos-e-inteligencia-artificial/dia-04)
- EU AI Act (Regulation 2024/1689)
- NIST AI Risk Management Framework (AI RMF 1.0)
- ISO/IEC 42001:2023 - Artificial Intelligence Management System
- OWASP Machine Learning Security Top 10
- MITRE ATLAS - Adversarial Threat Landscape for AI Systems
- Google Model Cards, Microsoft Datasheets for Datasets
- GDPR Art. 22 - Automated decision-making

## ‚úÖ Checklist de Implementaci√≥n

- [ ] Inventario de modelos de IA/ML en desarrollo y producci√≥n
- [ ] Clasificaci√≥n de riesgo seg√∫n AI Act o framework local
- [ ] Data Protection Impact Assessment (DPIA) para IA de alto riesgo
- [ ] Implementaci√≥n de data versioning (DVC)
- [ ] Anonimizaci√≥n/pseudonimizaci√≥n de datos de entrenamiento
- [ ] Testing adversarial en modelos cr√≠ticos
- [ ] Implementaci√≥n de differential privacy para datos sensibles
- [ ] Evaluaci√≥n de fairness en subgrupos demogr√°ficos
- [ ] Model cards documentados para todos los modelos
- [ ] Model registry con control de acceso
- [ ] API de inferencia con autenticaci√≥n y rate limiting
- [ ] Monitoreo de data drift y concept drift
- [ ] Alertas de degradaci√≥n de performance
- [ ] SHAP/LIME implementado para explicabilidad
- [ ] Proceso de auditor√≠a de modelos establecido
- [ ] Capacitaci√≥n en AI Security a equipos de Data Science
